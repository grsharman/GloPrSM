{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3edebf-9905-4612-8d25-5a45dce30ee0",
   "metadata": {},
   "source": [
    "## Code accompanyment to\"Machine learning applied to a modern-Pleistocene petrographic dataset: The global prediction of sand modal composition (GloPrSM) model\"\n",
    "### J. Isaac Johnson, Glenn R. Sharman, Eugene Szymanski, and Xiao Huang\n",
    "### University of Arkansas, Department of Geosciences\n",
    "### Please direct questions and correspondance to gsharman@uark.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d40f4d",
   "metadata": {},
   "source": [
    "## Step 1: Load sand modal composition data and make random forests models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f089d3d-e5e6-4772-8f02-172cabf12034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, train_test_split#, cross_validate, cross_val_score\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "import pickle\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df9ab1-eefa-4d70-a396-40eec0ad977f",
   "metadata": {},
   "source": [
    "### Load dependent variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9facea19-3392-4851-af7d-059ab839ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = pd.read_excel('GloPrSM_Input_v1.0.xlsx', engine='openpyxl')\n",
    "\n",
    "print(len(rf_data))\n",
    "rf_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd2892-571a-44ed-bbad-4173b3bb30ed",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01205cad-21e1-4055-a9b0-934feb7ac53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['Relief_AVG', 'Area_sq_km', 'Pre_mm_AVG', 'Tmp_dc_AVG', 'Slope_AVG', 'Lith_PYVAVI',\n",
    "                'Lith_PAPI', 'Lith_VB', 'Lith_PB', 'Lith_EVSC', 'Lith_SMSSSU', 'Lith_MT']\n",
    "\n",
    "features = rf_data[feature_list]\n",
    "features.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d02abe-a1b7-4884-a7df-67b76fe1bb9c",
   "metadata": {},
   "source": [
    "### Feature correlation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c39ab9-b243-47cb-a50e-db21c488407a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_list_clean=['Relief','Catchment\\n Area','Precipitation','Temperature','Slope','PY+VA+VI','PA+PI','VB','PB','EV+SC','SM+SS+SU','MT']\n",
    "corr = features.corr()\n",
    "top_corr_features = corr.index\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ax.axis('on'), ax.patch.set_edgecolor('black')\n",
    "g=sns.heatmap(features[top_corr_features].corr(),vmin=-1.,vmax=1.,xticklabels=feature_list_clean,yticklabels=feature_list_clean,annot=True,cmap=\"bwr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf26ed-249a-4c96-bdb6-fd6a90dfc837",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Note: this code can take many hours to run, depending on the number of splits chosen. A large amount of hard drive space is required (~2.5 GB per split used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3218f-f783-47d5-8c12-9c2c56c3f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output folder\n",
    "base_path = r'Z:\\Sharman\\GloPrSM_git\\v1.0'\n",
    "\n",
    "model_path = base_path + '\\\\' + 'models'\n",
    "val_path = base_path + '\\\\' + 'validation'\n",
    "test_path = base_path + '\\\\' + 'test_labels'\n",
    "\n",
    "# Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(val_path).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(test_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a25e2-ccb9-4639-aec5-72bb10c57451",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['FQ_QFL_IJ', 'LQ_QFL_IJ', 'QmQch_QmQpQch_IJ', 'QpQch_QmQpQch_IJ', 'FkFp_FpFk_IJ', 'LsLv_LvLsLm_IJ', 'LmLv_LvLsLm_IJ']\n",
    "\n",
    "splits = 10 # Note, 100 splits are used in the article\n",
    "rs = ShuffleSplit(n_splits=splits, test_size=.2, random_state=0)\n",
    "stats = np.zeros(shape=(splits,len(labels)))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for j, label in enumerate(labels):\n",
    "#for j, label in enumerate(labels[0:2]): # Just QFL\n",
    "    \n",
    "    val_df = pd.DataFrame()\n",
    "    tst_df = pd.DataFrame()\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=len(rf_data),random_state=0,max_features='auto')\n",
    "    data = rf_data[rf_data[label].isnull()==False].reset_index()\n",
    "    print(label,len(data))\n",
    "    \n",
    "    i = 0\n",
    "    for train_index, test_index in rs.split(data):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_features = data.iloc[train_index].loc[:, feature_list]\n",
    "        test_features  = data.iloc[test_index].loc[:, feature_list]\n",
    "        train_labels = data.iloc[train_index].loc[:,label]\n",
    "        test_labels  = data.iloc[test_index].loc[:,label]\n",
    "        \n",
    "        model.fit(train_features, train_labels)\n",
    "        prediction = model.predict(test_features)\n",
    "        r2 = r2_score(test_labels, prediction)\n",
    "\n",
    "        stats[i,j] = r2\n",
    "        val_df.loc[:,'{}_valid_{}'.format(label, i)] = prediction\n",
    "        tst_df.loc[:,'{}_label_{}'.format(label, i)] = test_labels\n",
    "\n",
    "        # Export the validation results\n",
    "        val_df.to_csv(val_path+'\\\\'+'{}_validation_rlf.csv'.format(label),index=False)\n",
    "        tst_df.to_csv(test_path+'\\\\'+'{}_label_rlf.csv'.format(label),index=False)\n",
    "        \n",
    "        # Save the model\n",
    "        model_filename = 'model_'+str(i)+'.sav'\n",
    "        model_filepath = model_path+'\\\\'+str(label)\n",
    "        pathlib.Path(model_filepath).mkdir(parents=True, exist_ok=True) # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "        pickle.dump(model, open(model_filepath+'\\\\'+model_filename, 'wb'))\n",
    "            \n",
    "        print(i, 'R2: {}, {} sec'.format(round(r2,6), round(time.time()-start,1)))    \n",
    "        i += 1\n",
    "    print()\n",
    "r2_df = pd.DataFrame(stats, columns=[x+'_R2' for x in labels])\n",
    "#r2_df.to_csv('/Volumes/G-DRIVE mobile SSD R-Series/Thesis/Variance_mapping/Glenn8/all_shuffles/Glenn_8_R2_stats.csv',index=False)\n",
    "r2_df.to_csv(base_path+'\\\\'+'R2_stats_rlf.csv',index=False)\n",
    "\n",
    "# Save the list of features used in the models, so you know what is going on\n",
    "features = pd.DataFrame()\n",
    "features['Inputs'] = feature_list\n",
    "features.to_csv(base_path+'\\\\'+'feature_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72be351-f83d-4c76-90c6-a0e93c34c082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
