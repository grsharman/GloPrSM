{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quiet-angel",
   "metadata": {},
   "source": [
    "## Code accompanyment to\"Machine learning applied to a modern-Pleistocene petrographic dataset: The global prediction of sand modal composition (GloPrSM) model\"\n",
    "### J. Isaac Johnson, Glenn R. Sharman, Eugene Szymanski, and Xiao Huang\n",
    "### University of Arkansas, Department of Geosciences\n",
    "### Please direct questions and correspondance to gsharman@uark.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-bridge",
   "metadata": {},
   "source": [
    "## Step 2: Load previously saved random forest models and generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-phrase",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.stats import skew\n",
    "import time\n",
    "from math import exp\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-universe",
   "metadata": {},
   "source": [
    "### Function to compute inverse log-ratio transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invLogRatio(data_t):\n",
    "    '''\n",
    "    Calculates the inverse log-ratio transformation of an array of any shape (Vermeesch, 2019, section 3, Eqn. 2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_t : array with dimensions (m, n) containing log-ratio data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_t_inv : inverse transformed array with dimensions (m, n+1)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Written by Isaac Johnson and Dr. Glenn Sharman (University of Arkansas)\n",
    "    '''\n",
    "    \n",
    "    # Compute inverse logratio to get back to compositional (sum-to-one) data\n",
    "    data_t_inv = np.zeros(shape=(data_t.shape[0],data_t.shape[1]+1))\n",
    "    denominator = 1.\n",
    "    \n",
    "    for i in range(data_t.shape[1]):\n",
    "        denominator += np.exp(data_t[:,i])\n",
    "    for i in range(data_t.shape[1]):\n",
    "        data_t_inv[:,i] = np.exp(data_t[:,i])/denominator\n",
    "    data_t_inv[:,-1] = 1/denominator\n",
    "    \n",
    "    return data_t_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-delicious",
   "metadata": {},
   "source": [
    "### Provide all inputs needed to run the code\n",
    "After this, the code should be able to run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output folder used in Step 1 (current directory used by default)\n",
    "base_path = os.getcwd() + '/v1.0'\n",
    "\n",
    "# Base directory of where to save results (recommended same as base_path)\n",
    "output_dir = os.getcwd() + '/v1.0'\n",
    "\n",
    "# CSV of independent variables in upstream mapped drainages\n",
    "wtrshd_ind_var = 'Watershed_output_lv8_ML_input.csv'\n",
    "\n",
    "# Feature list exported when saving the models in Step 1\n",
    "feature_list_dir = base_path + '/' + 'feature_list.csv'\n",
    "\n",
    "# The labels of the log ratios you would like to predict\n",
    "labels = ['FQ_QFL_IJ', 'LQ_QFL_IJ', 'QmQch_QmQpQch_IJ', 'QpQch_QmQpQch_IJ', 'FkFp_FpFk_IJ', 'LsLv_LvLsLm_IJ', 'LmLv_LvLsLm_IJ']\n",
    "\n",
    "# The dictionary for how labels relate to QFL and the 8 QFL subcompositions (Qm, Qp, Qch, Fk, Fp, Ls, Lv, Lm)\n",
    "systems_dict = {'QFL_IJ' : ['FQ_QFL_IJ', 'LQ_QFL_IJ'],\n",
    "                'QmQpQch' : ['QmQch_QmQpQch_IJ', 'QpQch_QmQpQch_IJ'],\n",
    "                'FpFk' : ['FkFp_FpFk_IJ'],\n",
    "                'LvLsLm' : ['LmLv_LvLsLm_IJ', 'LsLv_LvLsLm_IJ']\n",
    "               }\n",
    "\n",
    "final_columns = ['F','L','Q','Qm','Qp','Qch','Fk','Fp','Lm','Ls','Lv']\n",
    "methods = ['avg','med','std','skew','min','max','p025','p25','p75','p975']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-hungarian",
   "metadata": {},
   "source": [
    "### Load necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feature_list = list(pd.read_csv(feature_list_dir)['Inputs'])\n",
    "print('These are the inputs:',feature_list)\n",
    "\n",
    "wtrshd_data = pd.read_csv(wtrshd_ind_var)\n",
    "\n",
    "input_data = wtrshd_data[feature_list]\n",
    "print('There are',len(input_data.columns),'independent variables in the model')\n",
    "print('There are',len(input_data),'watersheds to be predicted')\n",
    "\n",
    "# Export the results\n",
    "input_data.to_csv(output_dir +'/' + 'input_data.csv') # Save the input data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-float",
   "metadata": {},
   "source": [
    "### Load models and generate predictions\n",
    "Note: This takes 2 to 8+ hours, depending on the number of watersheds you are predicting and the number of models generated in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-sewing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    models_to_import = []\n",
    "    os.chdir(base_path+'/'+'models'+'/'+label)\n",
    "\n",
    "    var_df = pd.DataFrame()\n",
    "\n",
    "    # Get the model file paths\n",
    "    for root, dirs, files in os.walk(base_path+'/'+'models'+'/'+label):\n",
    "        for file in glob.glob(\"*.sav\"):\n",
    "            models_to_import.append(os.path.join(root,file))\n",
    "            \n",
    "    # Load the models\n",
    "    for i in range(len(models_to_import)):\n",
    "        start = time.time()\n",
    "\n",
    "        model = pickle.load(open(models_to_import[i], 'rb'))\n",
    "        pred = model.predict(input_data)\n",
    "\n",
    "        var_df.loc[:,'{}_globe_{}'.format(label, i)] = pred\n",
    "        print('Finished:',models_to_import[i], 'Time: {} sec'.format(round(time.time()-start,1)))\n",
    "\n",
    "    # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "    pathlib.Path(output_dir+'/'+'variance_raw').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    var_df.to_csv(output_dir+'/'+'variance_raw'+'/{}_variance_raw_rlf.csv'.format(label),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-literature",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-instrument",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'/'+'variance_raw')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'/'+'variance_raw'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-slope",
   "metadata": {},
   "source": [
    "### Calculate log statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for file in files_to_import[:]:\n",
    "    \n",
    "    var_df = pd.DataFrame()\n",
    "    var_data = pd.read_csv(file)\n",
    "    ratio = file.split('/')[-1].split('_v')[0] # For Mac\n",
    "    #ratio = file.split('\\\\')[-1].split('_v')[0] # For PC\n",
    "    print('Currently calculating',ratio)\n",
    "    \n",
    "    var_df['HYBAS_ID'] = wtrshd_data['Wtrshd_ID']\n",
    "    var_df.loc[:,'{}_avg'.format(ratio)] = np.asarray(np.mean(var_data,axis=1))\n",
    "    var_df.loc[:,'{}_med'.format(ratio)] = np.median(var_data,axis=1)\n",
    "    var_df.loc[:,'{}_std'.format(ratio)] = np.asarray(np.std(var_data,axis=1))\n",
    "    var_df.loc[:,'{}_skew'.format(ratio)] = skew(var_data,axis=1)\n",
    "\n",
    "    runtime = np.zeros(len(wtrshd_data))\n",
    "    var_min = []\n",
    "    var_max = []\n",
    "    var_p025 = []\n",
    "    var_p250 = []\n",
    "    var_p750 = []\n",
    "    var_p975 = []\n",
    "    \n",
    "    for i in range(len(wtrshd_data)):\n",
    "        loop_start = time.time()\n",
    "        \n",
    "        var_min.append(var_data.loc[i].min())\n",
    "        var_max.append(var_data.loc[i].max())\n",
    "        var_p025.append(np.percentile(var_data.loc[i], q=2.5))\n",
    "        var_p250.append(np.percentile(var_data.loc[i], q=25))\n",
    "        var_p750.append(np.percentile(var_data.loc[i], q=75))\n",
    "        var_p975.append(np.percentile(var_data.loc[i], q=97.5))\n",
    "        \n",
    "        runtime[i]=time.time()-loop_start\n",
    "        if (i+1)%10000==0:\n",
    "            print(round(i/len(wtrshd_data)*100,4),'% completed.',round(np.mean(runtime[:i]),6),'seconds per basin')\n",
    "\n",
    "    var_df.loc[:,'{}_min'.format(ratio)] = var_min\n",
    "    var_df.loc[:,'{}_max'.format(ratio)] = var_max\n",
    "    var_df.loc[:,'{}_p025'.format(ratio)] = var_p025\n",
    "    var_df.loc[:,'{}_p25'.format(ratio)] = var_p250\n",
    "    var_df.loc[:,'{}_p75'.format(ratio)] = var_p750\n",
    "    var_df.loc[:,'{}_p975'.format(ratio)] = var_p975\n",
    "    \n",
    "    # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "    pathlib.Path(output_dir+'/'+'log_ratio_stats').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    var_df.to_csv(output_dir+'/'+'log_ratio_stats'+'/{}_stats.csv'.format(ratio),index=False)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-yorkshire",
   "metadata": {},
   "source": [
    "### Group log ratio stats by individual statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'/'+'log_ratio_stats')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'/'+'log_ratio_stats'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of files to import\n",
    "ratio_csvs = []\n",
    "for file in files_to_import:\n",
    "    ratio_csvs.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['avg','med','std','skew','min','max','p025','p25','p75','p975']\n",
    "ratios = [file.split('/')[-1].split('stats/')[0] for file in files_to_import]  # Mac\n",
    "#ratios = [file.split('\\\\')[-1].split('stats\\\\')[0] for file in files_to_import]  # PC\n",
    "ratios = [x[:(len(x)-10)] for x in ratios]\n",
    "\n",
    "var_data_dict = {'LR_data' : ratio_csvs}\n",
    "\n",
    "for key in var_data_dict:\n",
    "    for stat in stats:\n",
    "        print('Starting',stat)\n",
    "\n",
    "        method_df = pd.DataFrame()\n",
    "        method_df['HYBAS_ID'] = var_data_dict[key][0]['HYBAS_ID']\n",
    "\n",
    "        for i in range(len(ratios)):\n",
    "            LR = var_data_dict[key][i].loc[:,'{}_{}'.format(ratios[i], stat)]\n",
    "            method_df['{}_{}'.format(ratios[i],stat)] = LR\n",
    "        \n",
    "        print(stat, np.array(method_df).shape)\n",
    "        \n",
    "        # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "        pathlib.Path(output_dir+'/'+'log_ratio_by_stat_type').mkdir(parents=True, exist_ok=True)        \n",
    "        \n",
    "        method_df.to_csv(output_dir+'/'+'log_ratio_by_stat_type'+'/GloPrSM_LR_{}.csv'.format(stat),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-payment",
   "metadata": {},
   "source": [
    "### Inverse transform to ternary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'/'+'log_ratio_by_stat_type')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'/'+'log_ratio_by_stat_type'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "QFL = np.zeros(shape=(len(wtrshd_data),2))\n",
    "QmQpQch = np.zeros(shape=(len(wtrshd_data),2))\n",
    "FkFp = np.zeros(shape=(len(wtrshd_data),1))\n",
    "LmLsLv = np.zeros(shape=(len(wtrshd_data),2))\n",
    "\n",
    "all_ratios = [QFL, QmQpQch, FkFp, LmLsLv]\n",
    "oct_ratios = [QmQpQch, FkFp, LmLsLv]\n",
    "oct_tax = np.zeros(shape=(len(wtrshd_data),11))\n",
    "oct_tax_norm = np.zeros(shape=(len(wtrshd_data),11))\n",
    "\n",
    "# Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "pathlib.Path(output_dir+'/'+'IJ_QFL_relative_vals').mkdir(parents=True, exist_ok=True)    \n",
    "pathlib.Path(output_dir+'/'+'IJ_QFL_normalized_vals').mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "for i, file in enumerate(files_to_import[:]):\n",
    "    data = pd.read_csv(file)\n",
    "    method = file.split('_')[-1].split('.')[0]\n",
    "    print(method)\n",
    "    \n",
    "    index1, index2 = 0, 0\n",
    "    for j, key in enumerate(systems_dict): # iterate through each system\n",
    "        for k, item in enumerate(systems_dict[key]): # iterate through each system's log ratios\n",
    "            all_ratios[j][:,k] = data[item+'_'+method]\n",
    "        \n",
    "        inv_LR = invLogRatio(all_ratios[j])\n",
    "        index2+=inv_LR.shape[-1]\n",
    "        oct_tax[:,index1:index2] = inv_LR\n",
    "        \n",
    "        for index in range(index1,index2):\n",
    "            if index1 == 0:\n",
    "                oct_tax_norm[:,index] = inv_LR[:,index]\n",
    "            elif index1 == 3: # quartz\n",
    "                oct_tax_norm[:,index] = oct_tax[:,index]*oct_tax[:,2]\n",
    "            elif index1 == 6: # feldspar\n",
    "                oct_tax_norm[:,index] = oct_tax[:,index]*oct_tax[:,0]\n",
    "            elif index1 == 8: # lithics\n",
    "                oct_tax_norm[:,index] = oct_tax[:,index]*oct_tax[:,1]\n",
    "        \n",
    "        print(index1, index2)\n",
    "        index1+=inv_LR.shape[-1]\n",
    "    \n",
    "    var_df = pd.DataFrame(oct_tax, columns=[col+'_'+method for col in final_columns])\n",
    "    var_df_norm = pd.DataFrame(oct_tax_norm, columns=[col+'_'+method for col in final_columns])\n",
    "    var_df['HYBAS_ID'], var_df_norm['HYBAS_ID'] =  data['HYBAS_ID'], data['HYBAS_ID']\n",
    "    \n",
    "    var_df.to_csv(output_dir+'/'+'IJ_QFL_relative_vals'+'/octonary_var_{}.csv'.format(method),index=False)\n",
    "    var_df_norm.to_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_{}.csv'.format(method),index=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "p025 = pd.read_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_p025.csv')\n",
    "p25 = pd.read_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_p25.csv')\n",
    "p75 = pd.read_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_p75.csv')\n",
    "p975 = pd.read_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_p975.csv')\n",
    "inner_50 = pd.DataFrame()\n",
    "inner_95 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(p025.columns[:-1])):\n",
    "    print(final_columns[i])\n",
    "    inner_50.loc[:,final_columns[i]+'_i50'] = abs(p75.iloc[:,i]-p25.iloc[:,i])\n",
    "    inner_95.loc[:,final_columns[i]+'_i95'] = abs(p975.iloc[:,i]-p025.iloc[:,i])\n",
    "\n",
    "inner_50['HYBAS_ID'] = data['HYBAS_ID']\n",
    "inner_95['HYBAS_ID'] = data['HYBAS_ID']\n",
    "inner_50.to_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_i50.csv',index=False)\n",
    "inner_95.to_csv(output_dir+'/'+'IJ_QFL_normalized_vals'+'/octonary_var_i95.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-douglas",
   "metadata": {},
   "source": [
    "### Export results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'/'+'IJ_QFL_normalized_vals')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'/'+'IJ_QFL_normalized_vals'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_export = pd.DataFrame()\n",
    "for file in files_to_import:\n",
    "    new_data = pd.read_csv(file)\n",
    "    for col in new_data.columns:\n",
    "        df_to_export[col] = new_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_data = wtrshd_data.rename(columns={\"Wtrshd_ID\": \"HYBAS_ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only exporting the median and inner 95th percentile\n",
    "columns_to_export = ['HYBAS_ID','F_med','L_med','Q_med','Qm_med','Qp_med','Qch_med','Fk_med','Fp_med','Lm_med','Ls_med','Lv_med',\n",
    "                    'F_i95','L_i95','Q_i95','Qm_i95','Qp_i95','Qch_i95','Fk_i95','Fp_i95','Lm_i95','Ls_i95','Lv_i95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_merge = wtrshd_data[['HYBAS_ID','Area_sq_km']].merge(df_to_export[columns_to_export],on='HYBAS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_merge.to_csv(output_dir+'/'+'wtrshd_data_wQFL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-sydney",
   "metadata": {},
   "source": [
    "### Join results with the BasinATLAS level 08 shapefile and export as shapefile\n",
    "Note: you need to download the BasinATLAS v10 level-08 shapefile (~875 MB) from www.hydrosheds.org for the code below to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download the BasinATLAS (v10) level 8 shapefile and specify filepath below\n",
    "wtrshd_shp = gpd.read_file('BasinATLAS_v10_lev08.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_shp_wQFL = wtrshd_shp[['HYBAS_ID','geometry']].merge(wtrshd_merge, on='HYBAS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_shp_wQFL.to_file(output_dir+'/'+'wtrshd_shp_wQFL.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
