{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f9c902-0a7c-4901-9756-4f450f4ff88c",
   "metadata": {},
   "source": [
    "## Code accompanyment to\"Machine learning applied to a modern-Pleistocene petrographic dataset: The global prediction of sand modal composition (GloPrSM) model\"\n",
    "### J. Isaac Johnson, Glenn R. Sharman, Eugene Szymanski, and Xiao Huang\n",
    "### University of Arkansas, Department of Geosciences\n",
    "### Please direct questions and correspondance to gsharman@uark.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec820a",
   "metadata": {},
   "source": [
    "## Step 2: Load previously saved random forest models and generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de988e92",
   "metadata": {},
   "source": [
    "TODO: update once cleaned up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89220a-67d4-43c5-a0a9-cbd4f1fdd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.stats import skew\n",
    "import time\n",
    "from math import exp\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inverse log-ratio transform\n",
    "\n",
    "def invLogRatio(data_t):\n",
    "    '''\n",
    "    Calculates the inverse log-ratio transformation of an array of any shape (Vermeesch, 2019, section 3, Eqn. 2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_t : array with dimensions (m, n) containing log-ratio data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_t_inv : inverse transformed array with dimensions (m, n+1)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Written by Isaac Johnson and Dr. Glenn Sharman (University of Arkansas)\n",
    "    '''\n",
    "    \n",
    "    # Compute inverse logratio to get back to compositional (sum-to-one) data\n",
    "    data_t_inv = np.zeros(shape=(data_t.shape[0],data_t.shape[1]+1))\n",
    "    denominator = 1.\n",
    "    \n",
    "    for i in range(data_t.shape[1]):\n",
    "        denominator += np.exp(data_t[:,i])\n",
    "    for i in range(data_t.shape[1]):\n",
    "        data_t_inv[:,i] = np.exp(data_t[:,i])/denominator\n",
    "    data_t_inv[:,-1] = 1/denominator\n",
    "    \n",
    "    return data_t_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7c9a9-9440-4a32-9485-2e4090f0a683",
   "metadata": {},
   "source": [
    "### Provide all inputs needed to run the code\n",
    "After this, the code should be able to run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84350a8-8885-4ca5-a08c-e9173936dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output folder used in Step 1\n",
    "base_path = r'Z:\\Sharman\\GloPrSM_git\\v1.0'\n",
    "\n",
    "# Base directory of where to save results (recommended same as base_path)\n",
    "output_dir = r'Z:\\Sharman\\GloPrSM_git\\v1.0'\n",
    "\n",
    "# CSV of independent variables in upstream mapped drainages\n",
    "wtrshd_ind_var = 'Watershed_output_lv8_ML_input.csv'\n",
    "\n",
    "# Feature list exported when saving the models in Step 1\n",
    "feature_list_dir = base_path + '\\\\' + 'feature_list.csv'\n",
    "\n",
    "# The labels of the log ratios you would like to predict\n",
    "labels = ['FQ_QFL_IJ', 'LQ_QFL_IJ', 'QmQch_QmQpQch_IJ', 'QpQch_QmQpQch_IJ', 'FkFp_FpFk_IJ', 'LsLv_LvLsLm_IJ', 'LmLv_LvLsLm_IJ']\n",
    "\n",
    "# The dictionary for how labels relate to QFL and the 8 QFL subcompositions (Qm, Qp, Qch, K, P, Ls, Lv, Lm)\n",
    "systems_dict = {'QFL_IJ' : ['FQ_QFL_IJ', 'LQ_QFL_IJ'],\n",
    "                'QmQpQch' : ['QmQch_QmQpQch_IJ', 'QpQch_QmQpQch_IJ'],\n",
    "                'FpFk' : ['FkFp_FpFk_IJ'],\n",
    "                'LvLsLm' : ['LmLv_LvLsLm_IJ', 'LsLv_LvLsLm_IJ']\n",
    "               }\n",
    "\n",
    "final_columns = ['F','L','Q','Qm','Qp','Qch','Fk','Fp','Lm','Ls','Lv']\n",
    "methods = ['avg','med','std','skew','min','max','p025','p25','p75','p975']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea68a53-d4d3-4c6c-b778-86235ad5797b",
   "metadata": {},
   "source": [
    "### Load necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a93d5-6c36-403c-96df-822dfce6afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feature_list = list(pd.read_csv(feature_list_dir)['Inputs'])\n",
    "print('These are the inputs:',feature_list)\n",
    "\n",
    "wtrshd_data = pd.read_csv(wtrshd_ind_var)\n",
    "\n",
    "input_data = wtrshd_data[feature_list]\n",
    "print('There are',len(input_data.columns),'independent variables in the model')\n",
    "print('There are',len(input_data),'watersheds to be predicted')\n",
    "\n",
    "# Export the results\n",
    "input_data.to_csv(output_dir+'\\\\' + 'input_data.csv') # Save the input data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f09af4-996c-46d4-b114-cb3283377f9e",
   "metadata": {},
   "source": [
    "### Load models and made predictions\n",
    "Note: This takes 2 to 8+ hours, depending on the number of watersheds you are predicting and the number of models generated in Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407471a2-8b66-481e-be48-583d015667e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    models_to_import = []\n",
    "    os.chdir(base_path+'\\\\'+'models'+'\\\\'+label)\n",
    "\n",
    "    var_df = pd.DataFrame()\n",
    "\n",
    "    # Get the model file paths\n",
    "    for root, dirs, files in os.walk(base_path+'\\\\'+'models'+'\\\\'+label):\n",
    "        for file in glob.glob(\"*.sav\"):\n",
    "            models_to_import.append(os.path.join(root,file))\n",
    "            \n",
    "    # Load the models\n",
    "    for i in range(len(models_to_import)):\n",
    "        start = time.time()\n",
    "\n",
    "        model = pickle.load(open(models_to_import[i], 'rb'))\n",
    "        pred = model.predict(input_data)\n",
    "\n",
    "        var_df.loc[:,'{}_globe_{}'.format(label, i)] = pred\n",
    "        print('Finished:',models_to_import[i], 'Time: {} sec'.format(round(time.time()-start,1)))\n",
    "\n",
    "    # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "    pathlib.Path(output_dir+'\\\\'+'variance_raw').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    var_df.to_csv(output_dir+'\\\\'+'variance_raw'+'\\{}_variance_raw_rlf.csv'.format(label),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc57c4-022a-4b7d-b2e4-7ffd8cd13328",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4105978-2720-4681-a999-ac5740a534df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'\\\\'+'variance_raw')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'\\\\'+'variance_raw'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6006a7-77aa-49c8-ba9b-f2fb44d42d56",
   "metadata": {},
   "source": [
    "### Calculate log statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbbc5f-c0a1-4ebd-92f1-cc613927845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for file in files_to_import[:]:\n",
    "    \n",
    "    var_df = pd.DataFrame()\n",
    "    var_data = pd.read_csv(file)\n",
    "    #ratio = file.split('/')[-1].split('_v')[0] # For Mac\n",
    "    ratio = file.split('\\\\')[-1].split('_v')[0] # For PC\n",
    "    print('Currently calculating',ratio)\n",
    "    \n",
    "    var_df['HYBAS_ID'] = wtrshd_data['Wtrshd_ID']\n",
    "    var_df.loc[:,'{}_avg'.format(ratio)] = np.asarray(np.mean(var_data,axis=1))\n",
    "    var_df.loc[:,'{}_med'.format(ratio)] = np.median(var_data,axis=1)\n",
    "    var_df.loc[:,'{}_std'.format(ratio)] = np.asarray(np.std(var_data,axis=1))\n",
    "    var_df.loc[:,'{}_skew'.format(ratio)] = skew(var_data,axis=1)\n",
    "\n",
    "    runtime = np.zeros(len(wtrshd_data))\n",
    "    var_min = []\n",
    "    var_max = []\n",
    "    var_p25 = []\n",
    "    var_p250 = []\n",
    "    var_p750 = []\n",
    "    var_p975 = []\n",
    "    for i in range(len(wtrshd_data)):\n",
    "        loop_start = time.time()\n",
    "        \n",
    "        var_min.append(var_data.loc[i].min())\n",
    "        var_max.append(var_data.loc[i].max())\n",
    "        var_p25.append(np.percentile(var_data.loc[i], q=2.5))\n",
    "        var_p250.append(np.percentile(var_data.loc[i], q=25))\n",
    "        var_p750.append(np.percentile(var_data.loc[i], q=75))\n",
    "        var_p975.append(np.percentile(var_data.loc[i], q=97.5))\n",
    "\n",
    "        # Isaac's way of coding this up, resulted in problems\n",
    "        #var_df.loc[i,'{}_min'.format(ratio)] = var_data.loc[i,:].min()\n",
    "        #var_df.loc[i,'{}_max'.format(ratio)] = var_data.loc[i,:].max()\n",
    "        #var_df.loc[i,'{}_p025'.format(ratio)] = np.percentile(var_data.loc[i,:], q=2.5)\n",
    "        #var_df.loc[i,'{}_p25'.format(ratio)] = np.percentile(var_data.loc[i,:], q=25)\n",
    "        #var_df.loc[i,'{}_p75'.format(ratio)] = np.percentile(var_data.loc[i,:], q=75)\n",
    "        #var_df.loc[i,'{}_p975'.format(ratio)] = np.percentile(var_data.loc[i,:], q=97.5)\n",
    "\n",
    "        runtime[i]=time.time()-loop_start\n",
    "        if (i+1)%10000==0:\n",
    "            print(round(i/len(wtrshd_data)*100,4),'% completed.',round(np.mean(runtime[:i]),6),'seconds per basin')\n",
    "\n",
    "    var_df.loc[:,'{}_min'.format(ratio)] = var_min\n",
    "    var_df.loc[:,'{}_max'.format(ratio)] = var_max\n",
    "    var_df.loc[:,'{}_p025'.format(ratio)] = var_p25\n",
    "    var_df.loc[:,'{}_p25'.format(ratio)] = var_p250\n",
    "    var_df.loc[:,'{}_p75'.format(ratio)] = var_p750\n",
    "    var_df.loc[:,'{}_p975'.format(ratio)] = var_p975\n",
    "    \n",
    "    # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "    pathlib.Path(output_dir+'\\\\'+'log_ratio_stats').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    var_df.to_csv(output_dir+'\\\\'+'log_ratio_stats'+'\\{}_stats.csv'.format(ratio),index=False)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e12e74-f72a-4359-af31-df9f23ec775b",
   "metadata": {},
   "source": [
    "### Group log ratio stats by individual statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7fb4f-f356-4364-9cc4-5334dca6c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'\\\\'+'log_ratio_stats')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'\\\\'+'log_ratio_stats'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd9693-b172-48f5-91a7-f711a3d3ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of files to import\n",
    "ratio_csvs = []\n",
    "for file in files_to_import:\n",
    "    ratio_csvs.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed2a52-bdca-496f-95fc-e6eae4d68b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['avg','med','std','skew','min','max','p025','p25','p75','p975']\n",
    "ratios = [file.split('\\\\')[-1].split('stats\\\\')[0] for file in files_to_import]  # PC\n",
    "ratios = [x[:(len(x)-10)] for x in ratios]\n",
    "\n",
    "var_data_dict = {'LR_data' : ratio_csvs}\n",
    "\n",
    "for key in var_data_dict:\n",
    "    for stat in stats:\n",
    "        print('Starting',stat)\n",
    "\n",
    "        method_df = pd.DataFrame()\n",
    "        method_df['HYBAS_ID'] = var_data_dict[key][0]['HYBAS_ID']\n",
    "\n",
    "        for i in range(len(ratios)):\n",
    "            LR = var_data_dict[key][i].loc[:,'{}_{}'.format(ratios[i], stat)]\n",
    "            method_df['{}_{}'.format(ratios[i],stat)] = LR\n",
    "        \n",
    "        print(stat, np.array(method_df).shape)\n",
    "        \n",
    "        # Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "        pathlib.Path(output_dir+'\\\\'+'log_ratio_by_stat_type').mkdir(parents=True, exist_ok=True)        \n",
    "        \n",
    "        method_df.to_csv(output_dir+'\\\\'+'log_ratio_by_stat_type'+'\\GloPrSM_LR_{}.csv'.format(stat),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef556bc-66ff-4cdb-8cc4-81dbc026a9be",
   "metadata": {},
   "source": [
    "### Inverse transform to ternary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d9c37-5ee0-4375-9b4c-b85dbf63f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'\\\\'+'log_ratio_by_stat_type')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'\\\\'+'log_ratio_by_stat_type'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bce856-07c1-4d52-803c-185b643c923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QFL = np.zeros(shape=(len(wtrshd_data),2))\n",
    "QmQpQch = np.zeros(shape=(len(wtrshd_data),2))\n",
    "KP = np.zeros(shape=(len(wtrshd_data),1))\n",
    "LmLsLv = np.zeros(shape=(len(wtrshd_data),2))\n",
    "\n",
    "all_ratios = [QFL, QmQpQch, KP, LmLsLv]\n",
    "Glenn_8_ratios = [QmQpQch, KP, LmLsLv]\n",
    "Glenn8_tax = np.zeros(shape=(len(wtrshd_data),11))\n",
    "Glenn8_tax_norm = np.zeros(shape=(len(wtrshd_data),11))\n",
    "\n",
    "# Recursively creates the directory and does not raise an exception if the directory already exists\n",
    "pathlib.Path(output_dir+'\\\\'+'IJ_QFL_relative_vals').mkdir(parents=True, exist_ok=True)    \n",
    "pathlib.Path(output_dir+'\\\\'+'IJ_QFL_normalized_vals').mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "for i, file in enumerate(files_to_import[:]):\n",
    "    data = pd.read_csv(file)\n",
    "    method = file.split('_')[-1].split('.')[0]\n",
    "    print(method)\n",
    "    \n",
    "    index1, index2 = 0, 0\n",
    "    for j, key in enumerate(systems_dict): # iterate through each system\n",
    "        for k, item in enumerate(systems_dict[key]): # iterate through each system's log ratios\n",
    "            all_ratios[j][:,k] = data[item+'_'+method]\n",
    "        \n",
    "        inv_LR = invLogRatio(all_ratios[j])\n",
    "        index2+=inv_LR.shape[-1]\n",
    "        Glenn8_tax[:,index1:index2] = inv_LR\n",
    "        \n",
    "        for index in range(index1,index2):\n",
    "            if index1 == 0:\n",
    "                Glenn8_tax_norm[:,index] = inv_LR[:,index]\n",
    "            elif index1 == 3: # quartz\n",
    "                Glenn8_tax_norm[:,index] = Glenn8_tax[:,index]*Glenn8_tax[:,2]\n",
    "            elif index1 == 6: # feldspar\n",
    "                Glenn8_tax_norm[:,index] = Glenn8_tax[:,index]*Glenn8_tax[:,0]\n",
    "            elif index1 == 8: # lithics\n",
    "                Glenn8_tax_norm[:,index] = Glenn8_tax[:,index]*Glenn8_tax[:,1]\n",
    "        \n",
    "        print(index1,index2)\n",
    "        index1+=inv_LR.shape[-1]\n",
    "    \n",
    "    var_df = pd.DataFrame(Glenn8_tax,columns=[col+'_'+method for col in final_columns])\n",
    "    var_df_norm = pd.DataFrame(Glenn8_tax_norm,columns=[col+'_'+method for col in final_columns])\n",
    "    var_df['HYBAS_ID'], var_df_norm['HYBAS_ID'] =  data['HYBAS_ID'], data['HYBAS_ID']\n",
    "    \n",
    "    var_df.to_csv(output_dir+'\\\\'+'IJ_QFL_relative_vals'+'\\Glenn_8_var_{}.csv'.format(method),index=False)\n",
    "    var_df_norm.to_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_{}.csv'.format(method),index=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e0502-9284-4558-903d-09c1e09ebbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p025 = pd.read_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_p025.csv')\n",
    "p25 = pd.read_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_p25.csv')\n",
    "p75 = pd.read_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_p75.csv')\n",
    "p975 = pd.read_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_p975.csv')\n",
    "inner_50 = pd.DataFrame()\n",
    "inner_95 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(p025.columns[:-1])):\n",
    "    print(final_columns[i])\n",
    "    inner_50.loc[:,final_columns[i]+'_i50'] = abs(p75.iloc[:,i]-p25.iloc[:,i])\n",
    "    inner_95.loc[:,final_columns[i]+'_i95'] = abs(p975.iloc[:,i]-p025.iloc[:,i])\n",
    "\n",
    "inner_50['HYBAS_ID'] = data['HYBAS_ID']\n",
    "inner_95['HYBAS_ID'] = data['HYBAS_ID']\n",
    "inner_50.to_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_i50.csv',index=False)\n",
    "inner_95.to_csv(output_dir+'\\\\'+'IJ_QFL_normalized_vals'+'\\Glenn_8_var_i95.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaeccf-0b2f-4a52-928b-39cb7ee7eeea",
   "metadata": {},
   "source": [
    "### Export results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c97ad2-4670-4b13-a461-e4a436c283ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(output_dir+'\\\\'+'IJ_QFL_normalized_vals')\n",
    "files_to_import = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir+'\\\\'+'IJ_QFL_normalized_vals'):\n",
    "    for file in glob.glob(\"*.csv\"):\n",
    "        files_to_import.append(os.path.join(root,file))\n",
    "\n",
    "print('>>> files to import:', len(files_to_import))\n",
    "files_to_import[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02528d73-1eda-445e-9dc9-03adade0f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_export = pd.DataFrame()\n",
    "for file in files_to_import:\n",
    "    new_data = pd.read_csv(file)\n",
    "    for col in new_data.columns:\n",
    "        df_to_export[col] = new_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcca181",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_data = wtrshd_data.rename(columns={\"Wtrshd_ID\": \"HYBAS_ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c331275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only exporting the median and inner 95th percentile\n",
    "columns_to_export = ['HYBAS_ID','F_med','L_med','Q_med','Qm_med','Qp_med','Qch_med','Fk_med','Fp_med','Lm_med','Ls_med','Lv_med',\n",
    "                    'F_i95','L_i95','Q_i95','Qm_i95','Qp_i95','Qch_i95','Fk_i95','Fp_i95','Lm_i95','Ls_i95','Lv_i95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3029df-0341-48b7-8c26-5554c741e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_merge = wtrshd_data[['HYBAS_ID','Area_sq_km']].merge(df_to_export[columns_to_export],on='HYBAS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e33ba-9734-4e56-8ca0-b67735912675",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_merge.to_csv(output_dir+'\\\\'+'wtrshd_data_wQFL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a60285",
   "metadata": {},
   "source": [
    "### Join results with the BasinATLAS level 08 shapefile and export as shapefile\n",
    "Note: you need to download the BasinATLAS v10 level-08 shapefile (~875 MB) from www.hydrosheds.org for the code below to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download the BasinATLAS (v10) level 8 shapefile and specify filepath below\n",
    "wtrshd_shp = gpd.read_file('Z:\\Sharman\\GloPrSM_git\\BasinATLAS_v10_lev08.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_shp_wQFL = wtrshd_shp[['HYBAS_ID','geometry']].merge(wtrshd_merge, on='HYBAS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a09d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtrshd_shp_wQFL.to_file(output_dir+'\\\\'+'wtrshd_shp_wQFL.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b63b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
